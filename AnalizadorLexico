import ply.lex as lex

tokens = ('IDENTIFICADOR', 'ENTERO', 'DECIMAL', 'CADENA', 'ASIGNACION', 'SUMA', 'RESTA', 'MULTIPLICACION', 'DIVISION', 'IGUALDAD', 'DESIGUALDAD', 'MAYORQUE', 'MENORQUE', 'YLOGICO', 'OLOGICO', 'NEGACION', 'COMA', 'PARENTESISAPERTURA', 'PARENTESISCIERRE', 'LLAVEAPERTURA', 'LLAVECIERRE', 'CORCHETEAPERTURA', 'CORCHETECERRADURA', 'COMENTARIOUNALINEA', 'COMENTARIOMULTILINEA')

# Palabras clave

reserved = {
  'si' : 'SI',
  'sino' : 'SINO',
  'mientras' : 'MIENTRAS',
  'para' : 'PARA',
  'funcion' : 'DEFINIR',
  'retornar' : 'RETORNAR',
  'verdadero' : 'VERDADERO',
  'falso' : 'FALSO',
  'imprimir' : 'IMPRIMIR'
}

tokens = tokens + tuple(reserved.values())

# Expresiones Regulares de tokens simples

t_ASIGNACION = r'='
t_SUMA = r'\+'
t_RESTA = r'-'
t_MULTIPLICACION = r'\*'
t_DIVISION = r'/'
t_IGUALDAD = r'=='
t_DESIGUALDAD = r'!='
t_MAYORQUE = r'>'
t_MENORQUE = r'<'
t_YLOGICO = r'y'
t_OLOGICO = r'o'
t_NEGACION = r'no'
t_COMA = r','
t_PARENTESISAPERTURA = r'\('
t_PARENTESISCIERRE = r'\)'
t_LLAVEAPERTURA = r'\{'
t_LLAVECIERRE = r'\}'
t_CORCHETEAPERTURA = r'\['
t_CORCHETECERRADURA = r'\]'

# Expresiones Regulares de tokens complejos

# Identificadores (variables y funciones)
def t_IDENTIFICADOR(t):
    r'[a-zA-Z_][a-zA-Z0-9_]*'  # Inician con letra o _, pueden contener números
    t.type = reserved.get(t.value, 'IDENTIFICADOR')  # Si es palabra clave, se cambia el tipo
    return t

# Números enteros
def t_ENTERO(t):
    r'\d+'  # Secuencia de dígitos
    t.value = int(t.value)  # Convertir a entero
    return t

# Números decimales
def t_DECIMAL(t):
    r'\d+\.\d+'  # Números con punto decimal
    t.value = float(t.value)  # Convertir a flotante
    return t

# Cadenas de texto
def t_CADENA(t):
    r'"([^"\\]*(\\.[^"\\]*)*)"'  # Expresión regular para cadenas entre comillas dobles
    t.value = t.value[1:-1]  # Eliminar comillas externas
    return t

# Comentarios de una línea
def t_COMENTARIOUNALINEA(t):
    r'//.*'  # Cualquier cosa después de //
    pass  # Ignorar el comentario

# Comentarios multilínea
def t_COMENTARIOMULTILINEA(t):
    r'/\*[\s\S]*?\*/'  # Todo lo que esté dentro de /* ... */
    pass  # Ignorar el comentario

# Ignorar espacios y tabulaciones
t_ignore = ' \t'

# Contar saltos de línea para seguimiento de posición
def t_newline(t):
    r'\n+'
    t.lexer.lineno += len(t.value)  # Contar líneas nuevas

# Función para calcular la columna del token
def find_column(input_text, token):
    """ Calcula la columna en la línea actual """
    last_newline = input_text.rfind('\n', 0, token.lexpos)  # Buscar el último salto de línea
    if last_newline < 0:
        return token.lexpos + 1  # Si no hay saltos de línea, la columna es la posición actual
    return token.lexpos - last_newline

# Manejo de errores en el análisis léxico
def t_error(t):
    print(f"Carácter ilegal '{t.value[0]}' en línea {t.lineno}, columna {find_column(t.lexer.lexdata, t)}")
    t.lexer.skip(1)  # Saltar el carácter ilegal y continuar

# Construcción del lexer
lexer = lex.lex()

# Leer archivo y analizarlo
with open("codigo_fuente.txt", "r", encoding="utf-8") as f:
    data = f.read()  # Leer el contenido del archivo

lexer.input(data)  # Cargar el código en el analizador léxico

# Guardar tokens en una lista de diccionarios
tokens_list = []
while True:
    tok = lexer.token()
    if not tok:
        break  # Si no hay más tokens, salir del bucle
    tokens_list.append({
        "type": tok.type,  # Tipo de token
        "lexeme": tok.value,  # Valor del token
        "line": tok.lineno,  # Línea en la que se encuentra
        "column": find_column(data, tok)  # Columna del token
    })

# Imprimir tokens generados
for token in tokens_list:
    print(token)  # Muestra cada token en formato diccionario
